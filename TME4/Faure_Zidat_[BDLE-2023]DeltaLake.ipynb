{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* Master DAC - BDLE\n",
        "* Author: Mohamed-Amine Baazizi\n",
        "* Affiliation: LIP6 - Faculté des Sciences - Sorbonne Université\n",
        "* Email: mohamed-amine.baazizi@lip6.fr\n",
        "* October 2023"
      ],
      "metadata": {
        "id": "GwjrdGLoXzly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outline"
      ],
      "metadata": {
        "id": "GunJfIR6YM_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Documentation\n",
        "\n",
        "* Delta https://docs.delta.io/0.4.0/delta-intro.html\n",
        "* API Reference https://docs.delta.io/0.4.0/api/python/index.html\n",
        "\n",
        "Organization\n",
        "* Demo1 ->3 and additional material: illustrate concepts of the lecture\n",
        "* Use cases: the exercice(s) to solve\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GpI9l0JZYRdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisite"
      ],
      "metadata": {
        "id": "csSU_SanYPJQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mW_DrY7ZlUj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9563dca-d7cd-40df-fa6e-3ec806302240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "delta-spark 2.4.0 requires pyspark<3.5.0,>=3.4.0, but you have pyspark 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade -q pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade -q delta-spark"
      ],
      "metadata": {
        "id": "_gTXq3KfqLjK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "local = \"local[*]\"\n",
        "appName = \"DeltaLake\"\n",
        "localConfig = SparkConf().setAppName(appName).setMaster(local).\\\n",
        "  set(\"spark.executor.memory\", \"6G\").\\\n",
        "  set(\"spark.driver.memory\",\"6G\").\\\n",
        "  set(\"spark.sql.catalogImplementation\",\"in-memory\").\\\n",
        "  set(\"spark.sql.extensions\",\"io.delta.sql.DeltaSparkSessionExtension\").\\\n",
        "  set(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\").\\\n",
        "  set(\"spark.jars.packages\",\"io.delta:delta-core_2.12:2.4.0\").\\\n",
        "  set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.config(conf = localConfig).getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sc.setLogLevel(\"ERROR\")"
      ],
      "metadata": {
        "id": "ZGcYc-DKqpUi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### imports"
      ],
      "metadata": {
        "id": "NsDqSpCyp5gJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from delta.tables import *\n",
        "from pyspark.sql.functions import *\n"
      ],
      "metadata": {
        "id": "Mu3bInuxp7PY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo1"
      ],
      "metadata": {
        "id": "PPmP7vcOr6XX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load the data into delta"
      ],
      "metadata": {
        "id": "6sBb4C8GteZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.range(0, 5)\n",
        "data.write.format(\"delta\").save(\"/tmp/delta-table\")"
      ],
      "metadata": {
        "id": "zl4hoy44r88v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "770df20c-d40f-4283-8594-71b81d6a765b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-42269ec0786d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/delta-table\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1396\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1398\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minsertInto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Cannot write to already existent path file:/tmp/delta-table without setting OVERWRITE = 'true'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "97auVNGZsIIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### update the data\n",
        "#### overwrite"
      ],
      "metadata": {
        "id": "7gC5yWqIti5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.range(5, 10)\n",
        "data.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")"
      ],
      "metadata": {
        "id": "t8puykW0tLaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "NmkCfdMDy7ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### conditional overwrite"
      ],
      "metadata": {
        "id": "n51K-OE-zEbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deltaTable = DeltaTable.forPath(spark, \"/tmp/delta-table\")\n",
        "\n",
        "# Update every even value by adding 100 to it\n",
        "deltaTable.update(\n",
        "  condition = expr(\"id % 2 == 0\"),\n",
        "  set = { \"id\": expr(\"id + 100\") })\n"
      ],
      "metadata": {
        "id": "LF3fA9HQzHFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deltaTable.toDF().show()"
      ],
      "metadata": {
        "id": "uzAqgYXnzNh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete every even value\n",
        "deltaTable.delete(condition = expr(\"id % 2 == 0\"))\n",
        "\n",
        "deltaTable.toDF().show()"
      ],
      "metadata": {
        "id": "_E0P5eWHzn4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsert (merge) new data\n",
        "newData = spark.range(0, 20)\n",
        "\n",
        "deltaTable.alias(\"oldData\") \\\n",
        "  .merge(\n",
        "    newData.alias(\"newData\"),\n",
        "    \"oldData.id = newData.id\") \\\n",
        "  .whenMatchedUpdate(set = { \"id\": col(\"newData.id\") }) \\\n",
        "  .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") }) \\\n",
        "  .execute()\n",
        "\n",
        "deltaTable.toDF().show()"
      ],
      "metadata": {
        "id": "uIz0m0Lh0omn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### view history"
      ],
      "metadata": {
        "id": "LFk6LjuF6YLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = deltaTable.history()\n",
        "history.printSchema()\n",
        "history.show()"
      ],
      "metadata": {
        "id": "UGhaQsbq6e62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.select(\"version\",\"operation\",\"operationMetrics\").show(truncate=False)"
      ],
      "metadata": {
        "id": "gQAQjImGYMNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -al /tmp/delta-table/_delta_log"
      ],
      "metadata": {
        "id": "iSubzSaw9gHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /tmp/delta-table/_delta_log/00000000000000000000.json"
      ],
      "metadata": {
        "id": "f--HJ45Q9rLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/delta-table\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "h9PK7R8-6Ysu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/tmp/delta-table\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "Y25Av8869c53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datagen\n"
      ],
      "metadata": {
        "id": "8h6Zc1bPoAUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Persons"
      ],
      "metadata": {
        "id": "IQBLb81vr2_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create sample data\n",
        "data = [(\"12345\", \"Alice\", 25, \"123 Main St\"),\n",
        "        (\"67890\", \"Bob\", 30, \"456 Oak Ave\"),\n",
        "        (\"24680\", \"Charlie\", 35, \"789 Elm St\")]\n",
        "\n",
        "# create a DataFrame from the sample data\n",
        "df = spark.createDataFrame(data, [\"serial\", \"name\", \"age\", \"address\"])\n",
        "\n",
        "# write the DataFrame to Delta format\n",
        "df.write.format(\"delta\").save(\"/tmp/persons\")"
      ],
      "metadata": {
        "id": "thYVjfaEodqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newdata = [(\"78120\", \"Dan\", 42, \"432 Holly Rd\"),\n",
        "        (\"97362\", \"Lorry\", 40, \"290 Wise Ave\")]\n",
        "\n",
        "\n",
        "# create a DataFrame\n",
        "newPersons = spark.createDataFrame(newdata, [\"serial\", \"name\", \"age\", \"address\"])"
      ],
      "metadata": {
        "id": "Fhk2V0eNomjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Salaries"
      ],
      "metadata": {
        "id": "hrInpLvgod65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salaries = [(\"12345\", 45000),\n",
        "        (\"67890\", 52000),\n",
        "        (\"24680\", 36000),\n",
        "        (\"78120\", 60000),\n",
        "        (\"97362\",38000)]\n",
        "\n",
        "# create a DataFrame from the sample data\n",
        "df = spark.createDataFrame(salaries, [\"serial\", \"salary\"])\n",
        "\n",
        "# write the DataFrame to Delta format\n",
        "df.write.format(\"delta\").save(\"/tmp/salaries\")"
      ],
      "metadata": {
        "id": "aV6kh0SDowDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_salaries = [(\"12345\", 47000),\n",
        "        (\"67890\", 50000),\n",
        "        (\"24680\", 46000),\n",
        "        (\"78120\", 61000),\n",
        "        (\"97362\",39000)]\n",
        "\n",
        "# create a DataFrame\n",
        "newSalaries = spark.createDataFrame(new_salaries, [\"serial\", \"salary\"])"
      ],
      "metadata": {
        "id": "mPmbHiTso2Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sales"
      ],
      "metadata": {
        "id": "qvCkgOJ2o51k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales = [(\"CHA_2\",2,60),(\"BED_4\",1,300),(\"SHO_15\",2,60)]\n",
        "\n",
        "# create a DataFrame from the sample data\n",
        "df = spark.createDataFrame(sales, [\"product_id\", \"quantity\", \"totalprice\"])\n",
        "df.write.format(\"delta\").save(\"/tmp/sales\")"
      ],
      "metadata": {
        "id": "1DWKghO9o41u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sales = [(\"SHO_15\",3,90),(\"CHA_2\",1,30),(\"BED_6\",1,200)]\n",
        "\n",
        "newSales = spark.createDataFrame(new_sales, [\"product_id\", \"quantity\", \"totalprice\"])"
      ],
      "metadata": {
        "id": "0IpHcreyo-0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Products"
      ],
      "metadata": {
        "id": "29J3fU3po_v0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "products_list = [(\"CHA_2\",\"Furniture\",\"blue\"),(\"BED_4\",\"Furniture\",\"brown\"),(\"SHO_15\",\"Cloth\",\"black\")]\n",
        "products = spark.createDataFrame(products_list, [\"product_id\", \"category\", \"color\"])"
      ],
      "metadata": {
        "id": "nacySM1spCxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo2"
      ],
      "metadata": {
        "id": "OkgbLaJW-P0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. Adding new tuples\n",
        "Consider the Delta table `person` with the following columns: serial, name, age, and address. You have a new dataset `newPersons` with the same columns, but with additional records. Write a merge statement to update the Delta table with the new records.\n"
      ],
      "metadata": {
        "id": "aRP_QqBQN8W1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#load the persons table\n",
        "delta_persons = DeltaTable.forPath(spark, \"/tmp/persons\")\n",
        "\n",
        "# Define the merge condition\n",
        "merge_condition = \"target.serial = source.serial\"\n",
        "\n",
        "# Define the merge statement\n",
        "delta_persons.alias(\"target\").merge(\n",
        "    newPersons.alias(\"source\"), merge_condition\n",
        ").whenNotMatchedInsertAll().execute()\n",
        "\n"
      ],
      "metadata": {
        "id": "9xeqCgVS-R7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### verification"
      ],
      "metadata": {
        "id": "I6IlXToGeJs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta_persons.toDF().show()"
      ],
      "metadata": {
        "id": "vyyFYR7kdKnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delta_persons.history().select(\"version\",\"operation\",\"operationParameters\").show(truncate=False)"
      ],
      "metadata": {
        "id": "3KUlaJioolTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2: updating existing tuples\n",
        "Assume you have a Delta table `salaries` with columns serial and salary. You want to update the salary of the employees who earn less than 50,000. You have a new dataset, `newSalaries` with the same columns but with updated salary information. Write a merge statement to update the `salaries` table with the new salary information.\n"
      ],
      "metadata": {
        "id": "nKD875I8OYT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the persons table\n",
        "delta_salaries = DeltaTable.forPath(spark, \"/tmp/salaries\")\n",
        "\n",
        "# Define\n",
        "merge_condition = \"target.serial = source.serial and target.salary<50000\"\n",
        "update_expression = { \"salary\": \"source.salary\" }\n",
        "\n",
        "# merge statement\n",
        "delta_salaries.alias(\"target\") \\\n",
        "  .merge(\n",
        "    newSalaries.alias(\"source\"), merge_condition ) \\\n",
        "  .whenMatchedUpdate(set = update_expression) \\\n",
        "  .execute()\n"
      ],
      "metadata": {
        "id": "SA61XaZq-R0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### verification"
      ],
      "metadata": {
        "id": "bzM61vLjhzZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta_salaries.toDF().show()\n"
      ],
      "metadata": {
        "id": "EoIX_e18-Rro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delta_persons.history().select(\"version\",\"operation\",\"operationParameters\").show(truncate=False)"
      ],
      "metadata": {
        "id": "f1NJkYpEoHeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3: adding new tuples and updating existing ones\n",
        "You have a Delta table `sales` with columns `product_id`, `quantity`, and `totalprice`. Write a merge statement to insert the new products from a dataframe `newSales` into `sales` and to make sure that, for existing products `sales` has the sum of the quantity and totalprice.\n"
      ],
      "metadata": {
        "id": "hU_0ZWKHoqwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the sales table\n",
        "delta_sales = DeltaTable.forPath(spark, \"/tmp/sales\")"
      ],
      "metadata": {
        "id": "ls4YDHGNpzHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delta_sales.toDF().show()"
      ],
      "metadata": {
        "id": "S8TszdkmcSZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newSales.show()"
      ],
      "metadata": {
        "id": "TWKzbsl6cXrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define\n",
        "merge_condition = \"target.product_id = source.product_id\"\n",
        "update_expression = { \"quantity\": \"target.quantity+source.quantity\",  \"totalprice\": \"target.totalprice+source.totalprice\"}\n",
        "\n",
        "# merge statement\n",
        "delta_sales.alias(\"target\") \\\n",
        "  .merge(\n",
        "    newSales.alias(\"source\"), merge_condition ) \\\n",
        "  .whenMatchedUpdate(set = update_expression) \\\n",
        "  .whenNotMatchedInsertAll()\\\n",
        "  .execute()"
      ],
      "metadata": {
        "id": "JfklUBUsoq_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### verification"
      ],
      "metadata": {
        "id": "bV2Lv2VCssP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta_sales.toDF().show()"
      ],
      "metadata": {
        "id": "UYnSn2Fjsuld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delta_persons.history().select(\"version\",\"operation\",\"operationParameters\").show(truncate=False)"
      ],
      "metadata": {
        "id": "8_t2Gu-hynkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4: Merge tables with different schemas\n",
        "You have a Delta table `sales` with a column `product_id`, among other.  Write a merge statement to update `sales` with  information about products using a dataset `productInfo` which contains the columns `product_id`, `category` and `color`, when available."
      ],
      "metadata": {
        "id": "cOCzQqTmy7h9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "products.createOrReplaceTempView(\"products\")\n",
        "\n",
        "spark.sql(\"\"\"MERGE INTO delta.`/tmp/sales` t\n",
        "USING products s\n",
        "ON t.product_id = s.product_id\n",
        "WHEN MATCHED\n",
        "  THEN UPDATE SET *\n",
        "WHEN NOT MATCHED\n",
        "  THEN INSERT *\"\"\")"
      ],
      "metadata": {
        "id": "4puorchP4HTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"select * from  delta.`/tmp/sales` \"\"\").show()"
      ],
      "metadata": {
        "id": "bcCY3hLU5r1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### verification"
      ],
      "metadata": {
        "id": "XSjvt2O0y7TJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta_persons.history().select(\"version\",\"operation\",\"operationParameters\").show(truncate=False)"
      ],
      "metadata": {
        "id": "cwIyS0Zt5MSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo 3: Attaching constraints"
      ],
      "metadata": {
        "id": "D6Q_U_6R1yBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Not-null constraint"
      ],
      "metadata": {
        "id": "wWeLbIQj1-3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "CREATE TABLE default.persons (\n",
        "    serial INT NOT NULL,\n",
        "    name STRING,\n",
        "    birthDate TIMESTAMP,\n",
        "    address STRING\n",
        "  ) USING DELTA;\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "5TqMXDUD13mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" DESCRIBE DETAIL default.persons \"\"\").show()"
      ],
      "metadata": {
        "id": "fIyuOuYo4gLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/spark-warehouse/persons"
      ],
      "metadata": {
        "id": "AxYd9SN8jNgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"select * from default.persons \"\"\").show()\n"
      ],
      "metadata": {
        "id": "woPVn8U02uuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"insert into default.persons values (12345, \"Alice\",\"2000-02-01\" ,\"123 Main St\") \"\"\")"
      ],
      "metadata": {
        "id": "ILZAiGWN3HzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"select * from default.persons \"\"\").show()\n"
      ],
      "metadata": {
        "id": "C7yIbaHm3bmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[texte du lien](https://)Can we run the following statement?\n",
        "\n",
        "no there is a constraint on serial that must be an int not null"
      ],
      "metadata": {
        "id": "8ukalnEM3tw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#spark.sql(\"\"\"insert into default.persons values (null, \"Bob\",\"1996-03-14\" ,\"456 Oak Ave\") \"\"\")\n"
      ],
      "metadata": {
        "id": "1DYy3qy33euk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicate constraint"
      ],
      "metadata": {
        "id": "OCQ_46q02BDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" ALTER TABLE default.persons ADD CONSTRAINT birthdate CHECK (birthDate > '2000-01-01'); \"\"\")"
      ],
      "metadata": {
        "id": "yuhD3yaL2GU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"SHOW TBLPROPERTIES default.persons\"\"\").show(truncate=False)"
      ],
      "metadata": {
        "id": "wWsQJYGi4n0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can we run the following statement?"
      ],
      "metadata": {
        "id": "hESBMIS434pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"insert into default.persons values (47962, \"Bob\",\"2003-03-14\" ,\"456 Oak Ave\") \"\"\")"
      ],
      "metadata": {
        "id": "L2jaoF7H35Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spark.sql(\"\"\"insert into default.persons values (47962, \"Bob\",\"1999-03-14\" ,\"456 Oak Ave\") \"\"\")"
      ],
      "metadata": {
        "id": "O_zZuSuZlJPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use case 1"
      ],
      "metadata": {
        "id": "JgchT6SU7-xR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data import"
      ],
      "metadata": {
        "id": "Z1G5IVktAXSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://nuage.lip6.fr/s/BbQ9rzGHKJexKYp/download/sales.tar -O /tmp/sales.tar"
      ],
      "metadata": {
        "id": "8UXfY7fTbxFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /tmp/delta"
      ],
      "metadata": {
        "id": "IG0-ZghQr12K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! tar xvf /tmp/sales.tar -C /tmp/delta"
      ],
      "metadata": {
        "id": "Bqexa5alq3KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /tmp/delta/sales"
      ],
      "metadata": {
        "id": "ygK0YEv2soKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# original\n",
        "orginal_sales = spark.read.csv(\"/tmp/delta/sales/salesOriginal.csv\", header=True, inferSchema=True)\n",
        "orginal_sales.write.format(\"delta\").partitionBy(\"category\").save(\"/tmp/delta/deltaSales\")"
      ],
      "metadata": {
        "id": "4-Q3c1sDAbk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"count: %d \\n schema: \" % orginal_sales.count())\n",
        "orginal_sales.dtypes"
      ],
      "metadata": {
        "id": "5S55BMZquHRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# march 2023 sales\n",
        "march23_sales = spark.read.csv(\"/tmp/delta/sales/march23_sales.csv\", header=True, inferSchema=True)\n",
        "print(\"count: %d \\n schema: \" % march23_sales.count())\n",
        "march23_sales.dtypes"
      ],
      "metadata": {
        "id": "I19Lqa-puXaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the delta table"
      ],
      "metadata": {
        "id": "6e_agqb2v0n2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales = DeltaTable.forPath(spark, \"/tmp/delta/deltaSales\")"
      ],
      "metadata": {
        "id": "rIEBH42yv4VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.detail().show()"
      ],
      "metadata": {
        "id": "4gUD5wRXwCIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.toDF().show()"
      ],
      "metadata": {
        "id": "ggIo77dkouJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "march23_sales.show()"
      ],
      "metadata": {
        "id": "SqOrw0LepmYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.history().select(\"version\",\"operation\",\"operationParameters\").show(truncate=False)"
      ],
      "metadata": {
        "id": "P5Tdq-UwtVvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding new tuples\n",
        "Write a merge statement to include the march 2023 records into `deltaSales`"
      ],
      "metadata": {
        "id": "Dmo8PBuNAK_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merge_condition = \"target.saleid = source.saleid\"\n",
        "\n",
        "# merge statement\n",
        "deltaSales.alias(\"target\").merge(march23_sales.alias(\"source\"), merge_condition ).whenNotMatchedInsertAll().execute()\n"
      ],
      "metadata": {
        "id": "AIgF0VRn5Vhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.history().select(\"version\",\"operation\",\"operationMetrics\").show(truncate=False)"
      ],
      "metadata": {
        "id": "3CYcuXN7tNPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.toDF().count()"
      ],
      "metadata": {
        "id": "7mdtpQao5-wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Updating tuples\n",
        "Write an update statement that increases the prices of products sold on 2023, based on their category, as follows: furniture -> 05%, others -> 10%"
      ],
      "metadata": {
        "id": "jJ0GWq2OAR06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.toDF().where(\"saledate >= '2023-01-01' and category='Furniture'\").count()"
      ],
      "metadata": {
        "id": "1D8WrNtD6Hbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.toDF().show()"
      ],
      "metadata": {
        "id": "FCbDYlyhRIJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.update(condition = expr(\"saledate >= '2023-01-01' and category='Furniture'\"), set = { \"unitprice\": expr(\"unitprice + 5*unitprice/100\") })"
      ],
      "metadata": {
        "id": "S22dP28hAcj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.history().select(\"version\",\"operation\",\"operationParameters\").show()"
      ],
      "metadata": {
        "id": "TgmUtz4SvKVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.history().select(\"version\",\"operation\",\"operationMetrics\").show(truncate=False)"
      ],
      "metadata": {
        "id": "ntwjdpGHvUDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.toDF().where(\"saledate >= '2023-01-01' and category!='Furniture'\").count()"
      ],
      "metadata": {
        "id": "QmLGmK7A62K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.update(condition = expr(\"saledate >= '2023-01-01' and category!='Furniture'\"),set = { \"unitprice\": expr(\"unitprice + 10*unitprice/100\") })"
      ],
      "metadata": {
        "id": "vks75vtYvC-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.history().select(\"version\",\"operation\",\"operationMetrics\").show(truncate=False)"
      ],
      "metadata": {
        "id": "H3Ls5O-Gu_xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing old records\n",
        "remove all sales older than 01-Jan-2023. How many records remain?"
      ],
      "metadata": {
        "id": "hNyHm2oly6Cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.toDF().where(\"saledate < '2023-01-01'\").count()"
      ],
      "metadata": {
        "id": "_0Frr-Le7ZHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.delete(\"saledate < '2023-01-01'\")\n"
      ],
      "metadata": {
        "id": "tic8bxIgy95l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.history().select(\"version\",\"operation\",\"operationMetrics\").show(truncate=False)"
      ],
      "metadata": {
        "id": "0j1AgEbExiTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.toDF().count()"
      ],
      "metadata": {
        "id": "Q4vMAP8Ixq7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### History viewing\n",
        "Show the records that have been deleted. Use the metadata information and use dataframe operators.  "
      ],
      "metadata": {
        "id": "3y09Br-I04C8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.toDF().where(\"saledate < '2023-01-01'\").count()"
      ],
      "metadata": {
        "id": "Cs8B6igp08FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.format(\"delta\").option(\"versionAsOf\", 3).load(\"/tmp/delta/deltaSales\").where(\"saledate < '2023-01-01'\").count()\n"
      ],
      "metadata": {
        "id": "bATHAY9P8GV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vacuuming old records\n",
        "Permanently remove the deleted records using `vacuum`. Check the history again and make sure that the removal has been performed."
      ],
      "metadata": {
        "id": "n26NZ5ity-Rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.vacuum()"
      ],
      "metadata": {
        "id": "ZqmStJrEzBRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.history().select(\"version\",\"operation\",\"operationMetrics\").show(truncate=False)"
      ],
      "metadata": {
        "id": "eD_I143pyJkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -rhl /tmp/delta/deltaSales"
      ],
      "metadata": {
        "id": "PvyxnsCE-G0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deltaSales.toDF().count()"
      ],
      "metadata": {
        "id": "gePU423xzXA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "count the rows at version 0, 3 and 5 and analyse"
      ],
      "metadata": {
        "id": "A45gTr-Zu617"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = deltaSales.history()\n",
        "history.printSchema()\n",
        "history.show()\n",
        "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/delta/deltaSales\")\n",
        "print(df.count())"
      ],
      "metadata": {
        "id": "Ic9Q2t2MzfAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"delta\").option(\"versionAsOf\", 3).load(\"/tmp/delta/deltaSales\")\n",
        "df.count()"
      ],
      "metadata": {
        "id": "2XTgmJC10IeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"delta\").option(\"versionAsOf\", 5).load(\"/tmp/delta/deltaSales\")\n",
        "df.count()"
      ],
      "metadata": {
        "id": "f8FVSp2b0Enz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional material"
      ],
      "metadata": {
        "id": "4Pb2W4NytGqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generated columns\n",
        "Consider the sales data from the use case, create a delta table called `deltaSalesDate` with three additional columns `year`, `month` and `day` derived from the `saledate` column of the original data."
      ],
      "metadata": {
        "id": "8LnO5bVotKMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orginal_sales = spark.read.csv(\"/tmp/delta/sales/salesOriginal.csv\", header=True, inferSchema=True)\n",
        "orginal_sales.printSchema()"
      ],
      "metadata": {
        "id": "8FvLrVyJd4l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### option1: augment `orginal_sales` with the three columns"
      ],
      "metadata": {
        "id": "Zb9FOFoGe86i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "I-rI5VHSfz7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_with_date_components = orginal_sales.withColumn('year',year(col(\"saledate\")))\\\n",
        ".withColumn('month',month(col(\"saledate\")))\\\n",
        ".withColumn('day',dayofmonth(col(\"saledate\")))\n",
        "sales_with_date_components.show(10)"
      ],
      "metadata": {
        "id": "l59o77uttKlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_with_date_components.write.format(\"delta\").partitionBy(\"month\").save(\"/tmp/delta/deltaSalesDate\")"
      ],
      "metadata": {
        "id": "FcM3J8s0hmoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R /tmp/delta/deltaSalesDate"
      ],
      "metadata": {
        "id": "tPXVtHEyh9eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_with_date_components.write.format(\"delta\").partitionBy(\"year\",\"month\").save(\"/tmp/delta/deltaSalesDateBis\")"
      ],
      "metadata": {
        "id": "tGoelNOQiKx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R /tmp/delta/deltaSalesDateBis"
      ],
      "metadata": {
        "id": "_MtOXf4SiUvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### option2: create a delta table with a predefined schema\n",
        "https://docs.delta.io/latest/delta-batch.html#use-generated-columns"
      ],
      "metadata": {
        "id": "S6svXIoNfE74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DeltaTable.createOrReplace(spark) \\\n",
        "  .tableName(\"default.sales\") \\\n",
        "  .addColumn(\"saleid\", \"STRING\") \\\n",
        "  .addColumn(\"saledate\", \"TIMESTAMP\") \\\n",
        "  .addColumn(\"quantity\", \"INT\") \\\n",
        "  .addColumn(\"year\", \"INT\", generatedAlwaysAs=\"YEAR(saledate)\") \\\n",
        "  .addColumn(\"month\", \"INT\", generatedAlwaysAs=\"MONTH(saledate)\") \\\n",
        "  .addColumn(\"day\", \"INT\", generatedAlwaysAs=\"DAYOFMONTH(saledate)\") \\\n",
        "  .partitionedBy(\"year\", \"month\") \\\n",
        "  .execute()"
      ],
      "metadata": {
        "id": "0QsStMBFj0i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -R spark-warehouse/sales/\n"
      ],
      "metadata": {
        "id": "wCB--64Xk4at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" DESCRIBE DETAIL default.sales \"\"\").show()"
      ],
      "metadata": {
        "id": "W2LooyLLlHQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" select * from default.sales \"\"\").show()"
      ],
      "metadata": {
        "id": "BLSSMhAVl9EB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" insert into default.sales\n",
        "            values ('S000000124','2023-02-26 00:00:00',2.0,2023,02,26)  \"\"\").show()"
      ],
      "metadata": {
        "id": "v4eDy1I_mCrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" select * from default.sales \"\"\").show()"
      ],
      "metadata": {
        "id": "384n1_qmmsri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "if so run a query that aggregates on some measure like sum of `unitprice` based on `month` and observe the plan"
      ],
      "metadata": {
        "id": "0di-BAi3P1w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" DESCRIBE DETAIL delta.`/tmp/delta/deltaSalesDate` \"\"\").show()"
      ],
      "metadata": {
        "id": "Izdmkm5ToaJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" select * from delta.`/tmp/delta/deltaSalesDate` \"\"\").show(2)"
      ],
      "metadata": {
        "id": "Yage4ynmQVlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" select month, sum(unitprice) from delta.`/tmp/delta/deltaSalesDate` group by month \"\"\").show()"
      ],
      "metadata": {
        "id": "ya_wqt53n7EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.adaptive.enabled\",False)"
      ],
      "metadata": {
        "id": "CZkzfN-WpIC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" select month, sum(unitprice) from delta.`/tmp/delta/deltaSalesDate` group by month \"\"\").explain()"
      ],
      "metadata": {
        "id": "gj15YMTPoJju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run a different query that aggregates on some measure like sum of `unitprice` based on  `saledate` and compare the plan with the previous one"
      ],
      "metadata": {
        "id": "FXvSYw6zQV8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" select year, sum(unitprice) from delta.`/tmp/delta/deltaSalesDate` group by year \"\"\").show()"
      ],
      "metadata": {
        "id": "6eRThywqQhzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" select year, sum(unitprice) from delta.`/tmp/delta/deltaSalesDate` group by year \"\"\").explain()"
      ],
      "metadata": {
        "id": "Drr1YTFrp8o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retore a delta table to a previous state\n"
      ],
      "metadata": {
        "id": "X8Eddd50vdIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"DESCRIBE HISTORY  default.sales \"\"\").show()\n"
      ],
      "metadata": {
        "id": "6Dvpbr3jsGJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"SELECT * FROM default.sales VERSION AS OF 1;\"\"\").show()\n"
      ],
      "metadata": {
        "id": "Xlzrq5oOreXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use case2\n"
      ],
      "metadata": {
        "id": "Z0CbK4WzrAd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Table creation"
      ],
      "metadata": {
        "id": "QfZnO-YysdS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### sales table\n",
        "create a delta table `default.sales` with the following schema (saleid : String, saledate : Timestamp, productid: String, quantity : int, shopid : string)"
      ],
      "metadata": {
        "id": "9_wSOhsQsfMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DeltaTable.createOrReplace(spark) \\\n",
        "  .tableName(\"default.sales\") \\\n",
        "  .addColumn(\"saleid\", \"STRING\") \\\n",
        "  .addColumn(\"saledate\", \"TIMESTAMP\") \\\n",
        "  .addColumn(\"productid\", \"String\") \\\n",
        "  .addColumn(\"quantity\", \"INT\") \\\n",
        "  .addColumn(\"shopid\", \"string\") \\\n",
        "  .execute()"
      ],
      "metadata": {
        "id": "YLPjP-D1sgPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" DESCRIBE default.sales \"\"\").show()"
      ],
      "metadata": {
        "id": "tmya3KLZtN1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load the `/tmp/sales/march23_sales.csv` data into `default.sales` by selecting only the required columns"
      ],
      "metadata": {
        "id": "CzeAEIkRsgqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" select * from default.sales \"\"\").show()\n",
        "march23_sales.show()\n",
        "df.show()"
      ],
      "metadata": {
        "id": "5L2bSToLjnxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_select = [\"saleid\", \"saledate\", \"productid\", \"quantity\", \"shopid\"]\n",
        "march23_sales = march23_sales.select(columns_to_select)\n",
        "march23_sales.write.insertInto(\"default.sales\")"
      ],
      "metadata": {
        "id": "_zUwZ6yev266"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "see the result"
      ],
      "metadata": {
        "id": "AwBv3SPwucfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" select count (*) from default.sales \"\"\").show()"
      ],
      "metadata": {
        "id": "w_zzLbDVuZpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dates table\n",
        "create a delta table `default.dates` with the following schema (saledate: timestamp, year: int, month: int) by ensuring that year and month are extracted from saledate"
      ],
      "metadata": {
        "id": "r0XGzRwyuGFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DeltaTable.createOrReplace(spark) \\\n",
        "  .tableName(\"default.dates\") \\\n",
        "  .addColumn(\"saledate\", \"TIMESTAMP\") \\\n",
        "  .addColumn(\"year\", \"INT\", generatedAlwaysAs=\"YEAR(saledate)\") \\\n",
        "  .addColumn(\"month\", \"INT\", generatedAlwaysAs=\"MONTH(saledate)\") \\\n",
        "  .execute()"
      ],
      "metadata": {
        "id": "sj9fjQX3uG78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" DESCRIBE default.dates\"\"\").show()"
      ],
      "metadata": {
        "id": "aSLaq2UN5FWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "populate `default.dates` by inserting dates from `default.sales`"
      ],
      "metadata": {
        "id": "eWDOjPXluL-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.sql(\"\"\" SELECT saledate FROM default.sales \"\"\")\n",
        "df = df.withColumn('year',year(col(\"saledate\"))).withColumn('month',month(col(\"saledate\")))\n",
        "df.write.insertInto(\"default.dates\")"
      ],
      "metadata": {
        "id": "ouTopqlkuMj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "see the result"
      ],
      "metadata": {
        "id": "zEVZ7GxMueq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" select * from default.dates limit 10 \"\"\").show()"
      ],
      "metadata": {
        "id": "LMXXtrEruX4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### products table\n",
        "create a delta table default.products with the following schema (productid: string, unitprice: double, category: string, subcategory: string, size: string, color: string,\n",
        " brand: string) by extracting data from `originalSales.csv`"
      ],
      "metadata": {
        "id": "H5TR--LsuNHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the schema for the Delta table\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"productid\", StringType(), True),\n",
        "    StructField(\"unitprice\", DoubleType(), True),\n",
        "    StructField(\"category\", StringType(), True),\n",
        "    StructField(\"subcategory\", StringType(), True),\n",
        "    StructField(\"size\", StringType(), True),\n",
        "    StructField(\"color\", StringType(), True),\n",
        "    StructField(\"brand\", StringType(), True)\n",
        "])\n",
        "csv_file_path = \"/tmp/delta/sales/\"\n",
        "df = spark.read.option(\"header\", \"true\").schema(schema).csv(csv_file_path)\n",
        "\n",
        "# Write the DataFrame to a Delta Lake table\n",
        "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.products\")\n"
      ],
      "metadata": {
        "id": "-hhacqRfuP50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DeltaTable.createOrReplace(spark) \\\n",
        "  .tableName(\"default.product\") \\\n",
        "  .addColumn(\"productid\", \"STRING\") \\\n",
        "  .addColumn(\"unitprice\", \"DOUBLE\") \\\n",
        "  .addColumn(\"category\", \"STRING\") \\\n",
        "  .addColumn(\"subcategory\", \"STRING\") \\\n",
        "  .addColumn(\"size\", \"STRING\") \\\n",
        "  .addColumn(\"color\", \"STRING\") \\\n",
        "  .addColumn(\"brand\", \"STRING\") \\\n",
        "  .execute()"
      ],
      "metadata": {
        "id": "tUVLtfuH9bDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "see the result"
      ],
      "metadata": {
        "id": "L82ic8TRufac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" select * from default.products limit 10 \"\"\").show()"
      ],
      "metadata": {
        "id": "QcPg5Y-AuTB0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}